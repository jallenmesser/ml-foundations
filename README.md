# ML Foundations

This repository documents my return to core machine learning
and mathematical foundations as I prepare for AI-focused roles.

My goal is not speed or memorization, but deep understanding
through intuition, implementation, and explanation.

Topics I will cover include:
- Gradients and optimization
- Vectors and matrices
- Probability and expectation
- What it means for a model to learn

This repository will evolve as I learn.

## Gradient Descent (Intuition)

Gradient descent works by using local slope information
to decide which direction to move in order to minimize a function.

The gradient tells us which direction is uphill.
To move downhill, we step in the opposite direction.

The learning rate controls how large each step is.
If it's too small, learning is slow.
If it's too large, the process can oscillate and fail to converge.
